{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ciencias4.txt',\n",
       " 'deportes3.txt',\n",
       " 'deportes6.txt',\n",
       " 'ciencias6.txt',\n",
       " 'economia4.txt',\n",
       " 'ciencias3.txt',\n",
       " 'ciencias2.txt',\n",
       " 'deportes2.txt',\n",
       " 'economia5.txt',\n",
       " 'economia2.txt',\n",
       " 'deportes1.txt',\n",
       " 'deportes5.txt',\n",
       " 'economia3.txt',\n",
       " 'economia1.txt',\n",
       " 'ciencias1.txt',\n",
       " 'deportes4.txt',\n",
       " 'ciencias5.txt',\n",
       " 'economia6.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('noticias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = []\n",
    "for i, noti in enumerate(os.listdir('noticias')):\n",
    "    docs_list.append('')\n",
    "    with open(\"noticias/\"+noti) as f:\n",
    "        for line in f:\n",
    "            docs_list[i] = docs_list[i] + line.replace('\\n',' ')\n",
    "    f.close()\n",
    "len(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    for line in f:\n",
    "        stop_list.append(line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmo LDA usando un diccionario de palabras: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseg/miniconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1 \n",
      "Words: 0.070*\"compañía\" + 0.057*\"millones\" + 0.045*\"mercado\" + 0.040*\"firma\"\n",
      "Topic: 2 \n",
      "Words: 0.045*\"fútbol\" + 0.043*\"técnico\" + 0.035*\"hora\" + 0.033*\"selección\"\n",
      "Topic: 3 \n",
      "Words: 0.004*\"hombre\" + 0.004*\"economía\" + 0.004*\"negocio\" + 0.004*\"rivales\"\n",
      "Topic: 4 \n",
      "Words: 0.040*\"agujeros\" + 0.038*\"científicos\" + 0.036*\"einstein\" + 0.036*\"millones\"\n",
      "Topic: 5 \n",
      "Words: 0.046*\"américa\" + 0.034*\"fútbol\" + 0.033*\"ingresos\" + 0.030*\"países\"\n",
      "Topic: 6 \n",
      "Words: 0.083*\"internacional\" + 0.057*\"científicos\" + 0.051*\"base\" + 0.051*\"me\"\n",
      "Topic: 7 \n",
      "Words: 0.053*\"negocio\" + 0.040*\"economía\" + 0.040*\"hombre\" + 0.026*\"mercado\"\n",
      "Topic: 8 \n",
      "Words: 0.004*\"gravedad\" + 0.004*\"millones\" + 0.004*\"científicos\" + 0.004*\"einstein\"\n",
      "\n",
      "\n",
      "Algoritmo LDA usando un objeto tf-idf: \n",
      "Topic: 1 \n",
      "Word: 0.158*\"internacional\" + 0.064*\"radiación\" + 0.061*\"medida\" + 0.054*\"vivir\"\n",
      "Topic: 2 \n",
      "Word: 0.053*\"fútbol\" + 0.046*\"argentina\" + 0.040*\"técnico\" + 0.039*\"selección\"\n",
      "Topic: 3 \n",
      "Word: 0.029*\"ingresos\" + 0.024*\"agujeros\" + 0.023*\"científicos\" + 0.022*\"millones\"\n",
      "Topic: 4 \n",
      "Word: 0.064*\"firma\" + 0.064*\"empleados\" + 0.049*\"rivales\" + 0.043*\"éxito\"\n",
      "Topic: 5 \n",
      "Word: 0.096*\"hombre\" + 0.060*\"latina\" + 0.055*\"presidente\" + 0.042*\"mujer\"\n",
      "Topic: 6 \n",
      "Word: 0.044*\"américa\" + 0.044*\"persona\" + 0.041*\"mercado\" + 0.040*\"universidad\"\n",
      "Topic: 7 \n",
      "Word: 0.148*\"contenido\" + 0.084*\"compañía\" + 0.075*\"mercado\" + 0.050*\"firma\"\n",
      "Topic: 8 \n",
      "Word: 0.160*\"gravedad\" + 0.081*\"mile\" + 0.076*\"realmente\" + 0.071*\"obtener\"\n"
     ]
    }
   ],
   "source": [
    "doc_complete = docs_list\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from gensim import parsing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "#need to install this library with this comand ->\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "#stem might not be an actual word\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "#lemma is an actual language word\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "#stop word is a commonly used word (such as “the”, “in”) that a search engine has been programmed to ignore\n",
    "stop = set(stopwords.words('spanish'))\n",
    "stop = list(set().union(stop,stop_list))\n",
    "#puntuación a eliminar\n",
    "exclude = set().union(string.punctuation,'“”¿|/\\ºª@#~$€%½¬&{}[]()=`+*-¨´Çç_-:.;,¡!')\n",
    "\n",
    "#function to stem\n",
    "# def stemmer_stemming(text):\n",
    "#     return stemmer.stem(text)\n",
    "\n",
    "def clean(doc):\n",
    "#     print(doc)\n",
    "    #Quitamos las stop words y aplicamos metodo lower\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop and len(i) > 3])\n",
    "    #removemos la puntuancion\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in word_tokenize(punc_free))\n",
    "#     print(normalized)\n",
    "#     result = []\n",
    "#     for token in gensim.utils.simple_preprocess(normalized):\n",
    "#         if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "#             result.append(stemmer_stemming(token))\n",
    "#     print(result)\n",
    "    return normalized\n",
    "\n",
    "#limpiamos la lista de documentos\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "\n",
    "#Creamos un diccionario a partir de 'doc_clean' que contenga el número de veces que una palabra aparece\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#Filtramos los tokens que aparecen en:\n",
    "#     menos de n documentos (número absoluto) o\n",
    "#     más de % documentos (fracción del tamaño total del corpus, no número absoluto).\n",
    "#     después de los dos pasos anteriores, guarde sólo los primeros 5000 tokens más frecuentes.\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7, keep_n=1000)\n",
    "\n",
    "# Para cada documento creamos un diccionario de palabras con las veces que aparecen\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#################\n",
    "#check dictionary\n",
    "# print('Palabras que más aparecen: ')\n",
    "# counter_dic = 1\n",
    "# for i in doc_term_matrix:\n",
    "#     for k in i:\n",
    "#         if k[1] > 3:\n",
    "#             print(\"Palabra {} (\\\"{}\\\") aparece {} veces.\".format(k[0],\n",
    "#                                                          dictionary[k[0]],\n",
    "#                                                          k[1]))\n",
    "#         counter_dic +=1\n",
    "# # counter_dic\n",
    "#################\n",
    "Nro_topis = 8\n",
    "Nro_words = 4\n",
    "\n",
    "#algoritmo LDA usando un diccionario de palabras\n",
    "print(\"Algoritmo LDA usando un diccionario de palabras: \")\n",
    "Lda = gensim.models.LdaMulticore\n",
    "lda_model = Lda(doc_term_matrix, num_topics=Nro_topis, id2word = dictionary, passes=30, alpha=.01, eta=.01, workers=3)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(num_topics=Nro_topis, num_words=Nro_words):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx+1, topic))\n",
    "\n",
    "# lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "print()\n",
    "print(\"\")\n",
    "#creamos un objeto tf-idf \"Term frequency – Inverse document frequency\"\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "#algoritmo LDA usando un objeto tf-idf \"Term frequency – Inverse document frequency\"\n",
    "print(\"Algoritmo LDA usando un objeto tf-idf: \")\n",
    "lda_model_tfidf = Lda(corpus_tfidf, num_topics=Nro_topis, id2word = dictionary, passes=30, alpha=.0005, eta=.005, workers=3)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_topics=Nro_topis, num_words=Nro_words):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx+1, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando en test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_eco.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5114386677742004\t Topic: 0.070*\"compañía\" + 0.057*\"millones\" + 0.045*\"mercado\" + 0.040*\"firma\"\n",
      "Score: 0.33321481943130493\t Topic: 0.046*\"américa\" + 0.034*\"fútbol\" + 0.033*\"ingresos\" + 0.030*\"países\"\n",
      "Score: 0.09188761562108994\t Topic: 0.083*\"internacional\" + 0.057*\"científicos\" + 0.051*\"base\" + 0.051*\"me\"\n",
      "Score: 0.06266015022993088\t Topic: 0.040*\"agujeros\" + 0.038*\"científicos\" + 0.036*\"einstein\" + 0.036*\"millones\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.3965222239494324\t \n",
      "Topic: 0.029*\"ingresos\" + 0.024*\"agujeros\" + 0.023*\"científicos\" + 0.022*\"millones\"\n",
      "\n",
      "Score: 0.28827670216560364\t \n",
      "Topic: 0.044*\"américa\" + 0.044*\"persona\" + 0.041*\"mercado\" + 0.040*\"universidad\"\n",
      "\n",
      "Score: 0.12465716153383255\t \n",
      "Topic: 0.064*\"firma\" + 0.064*\"empleados\" + 0.049*\"rivales\" + 0.043*\"éxito\"\n",
      "\n",
      "Score: 0.10639659315347672\t \n",
      "Topic: 0.053*\"fútbol\" + 0.046*\"argentina\" + 0.040*\"técnico\" + 0.039*\"selección\"\n",
      "\n",
      "Score: 0.08411730825901031\t \n",
      "Topic: 0.096*\"hombre\" + 0.060*\"latina\" + 0.055*\"presidente\" + 0.042*\"mujer\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_dep.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5249103903770447\t Topic: 0.045*\"fútbol\" + 0.043*\"técnico\" + 0.035*\"hora\" + 0.033*\"selección\"\n",
      "Score: 0.3563339114189148\t Topic: 0.046*\"américa\" + 0.034*\"fútbol\" + 0.033*\"ingresos\" + 0.030*\"países\"\n",
      "Score: 0.11719703674316406\t Topic: 0.040*\"agujeros\" + 0.038*\"científicos\" + 0.036*\"einstein\" + 0.036*\"millones\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.43705153465270996\t \n",
      "Topic: 0.053*\"fútbol\" + 0.046*\"argentina\" + 0.040*\"técnico\" + 0.039*\"selección\"\n",
      "\n",
      "Score: 0.38421374559402466\t \n",
      "Topic: 0.029*\"ingresos\" + 0.024*\"agujeros\" + 0.023*\"científicos\" + 0.022*\"millones\"\n",
      "\n",
      "Score: 0.08766701817512512\t \n",
      "Topic: 0.096*\"hombre\" + 0.060*\"latina\" + 0.055*\"presidente\" + 0.042*\"mujer\"\n",
      "\n",
      "Score: 0.05971253290772438\t \n",
      "Topic: 0.064*\"firma\" + 0.064*\"empleados\" + 0.049*\"rivales\" + 0.043*\"éxito\"\n",
      "\n",
      "Score: 0.03130831941962242\t \n",
      "Topic: 0.158*\"internacional\" + 0.064*\"radiación\" + 0.061*\"medida\" + 0.054*\"vivir\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_test = ['']\n",
    "with open(\"test_cie.txt\") as f:\n",
    "    for line in f:\n",
    "        docs_list_test[0] = docs_list_test[0] + line.replace('\\n',' ')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean_test = [clean(doc).split() for doc in docs_list_test]\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in doc_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7169049382209778\t Topic: 0.040*\"agujeros\" + 0.038*\"científicos\" + 0.036*\"einstein\" + 0.036*\"millones\"\n",
      "Score: 0.19706152379512787\t Topic: 0.046*\"américa\" + 0.034*\"fútbol\" + 0.033*\"ingresos\" + 0.030*\"países\"\n",
      "Score: 0.0856606513261795\t Topic: 0.053*\"negocio\" + 0.040*\"economía\" + 0.040*\"hombre\" + 0.026*\"mercado\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = doc_term_matrix_test[0]\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lda_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6611242890357971\t \n",
      "Topic: 0.029*\"ingresos\" + 0.024*\"agujeros\" + 0.023*\"científicos\" + 0.022*\"millones\"\n",
      "\n",
      "Score: 0.10835292190313339\t \n",
      "Topic: 0.044*\"américa\" + 0.044*\"persona\" + 0.041*\"mercado\" + 0.040*\"universidad\"\n",
      "\n",
      "Score: 0.08942861109972\t \n",
      "Topic: 0.160*\"gravedad\" + 0.081*\"mile\" + 0.076*\"realmente\" + 0.071*\"obtener\"\n",
      "\n",
      "Score: 0.07408390194177628\t \n",
      "Topic: 0.053*\"fútbol\" + 0.046*\"argentina\" + 0.040*\"técnico\" + 0.039*\"selección\"\n",
      "\n",
      "Score: 0.04173697158694267\t \n",
      "Topic: 0.096*\"hombre\" + 0.060*\"latina\" + 0.055*\"presidente\" + 0.042*\"mujer\"\n",
      "\n",
      "Score: 0.019295262172818184\t \n",
      "Topic: 0.158*\"internacional\" + 0.064*\"radiación\" + 0.061*\"medida\" + 0.054*\"vivir\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, Nro_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:\n",
    "- deportes1 https://www.bbc.com/mundo/deportes-47874344\n",
    "- deportes2 https://www.bbc.com/mundo/deportes-47867626\n",
    "- deportes3 https://www.bbc.com/mundo/deportes-47867623\n",
    "- deportes4 https://www.bbc.com/mundo/deportes-47867621\n",
    "- deportes5 https://www.bbc.com/mundo/deportes-47827846\n",
    "- deportes6 https://www.bbc.com/mundo/noticias-47758928\n",
    "- ciencias1 https://www.bbc.com/mundo/noticias-47915625\n",
    "- ciencias2 https://www.bbc.com/mundo/noticias-47859720\n",
    "- ciencias3 https://www.bbc.com/mundo/noticias-47915620\n",
    "- ciencias4 https://www.bbc.com/mundo/noticias-47909000\n",
    "- ciencias5 https://www.bbc.com/mundo/noticias-47860116\n",
    "- ciencias6 https://www.bbc.com/mundo/noticias-47900442\n",
    "- economia1 https://www.bbc.com/mundo/noticias-47861934\n",
    "- economia2 https://www.bbc.com/mundo/noticias-47670796\n",
    "- economia3 https://www.bbc.com/mundo/noticias-47901139\n",
    "- economia4 https://www.bbc.com/mundo/noticias-47908922\n",
    "- economia5 https://www.bbc.com/mundo/noticias-47900832\n",
    "- economia6 https://www.bbc.com/mundo/noticias-47895086\n",
    "\n",
    "\n",
    "Test:\n",
    "- test_cie https://www.bbc.com/mundo/noticias-47884630\n",
    "- test dep https://www.bbc.com/mundo/deportes-47667089\n",
    "- test_eco https://www.bbc.com/mundo/noticias-47872006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
